{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021a38f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Dec 12 02:37:35 PM: Encountered unexpected exception importing solver GLOP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.8.3296). Expected < 9.8.0. Please open a feature request on cvxpy to enable support for this version.')\n",
      "(CVXPY) Dec 12 02:37:35 PM: Encountered unexpected exception importing solver PDLP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.8.3296). Expected < 9.8.0. Please open a feature request on cvxpy to enable support for this version.')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BinningProces' from 'optbinning' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\optbinning\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13712/2447977425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0moptbinning\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptimalBinning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScorecard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinningProces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfunction_for_eda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BinningProces' from 'optbinning' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\optbinning\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from optbinning import OptimalBinning, Scorecard, BinningProces\n",
    "import import_ipynb\n",
    "from function_for_eda import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254dc177",
   "metadata": {},
   "source": [
    "# 1. APPLICATION_TRAIN.CSV AND APPLICATION_TEST.CSV\n",
    "Các bảng application_train và application_test chứa các thông tin cá nhân trực tiếp liên quan đến những người đi vay vì vậy đây là bảng chính và quan trọng nhất. Từ các bước EDA trước đó, chúng ta tiến hành các bước Feature Engineering như sau (một điều lưu ý rằng tất cả các bước khi thực hiện ở bảng application_train thì đều phải thực hiện tương tự ở bảng application_test):\n",
    "- <b>Bước 1</b>: Xóa các cột không liên quan 'Unnamed: 0', xử lý các giá trị bất thường bằng cách thay thế nó bằng Nan\n",
    "- <b>Bước 2</b>: Thực hiện các bước data processing cơ bản như xóa đi các cột chứa missing value trên 60%, fill các giá trị nan, chuyển các cột chứa giá trị ngày thành năm, xóa các cột chỉ chứa 1 giá trị, xử lý các giá trị outliers\n",
    "- <b>Bước 3</b>: Tạo một số feature mới dựa trên Domain Knowledge, qua trình EDA và một số solutions tham khảo khác\n",
    "- <b>Bước 4</b>: Sử dụng BinningProcess từ thư viện optbinning để tìm cách phân nhóm tối ưu cho một biến nhất định (ở đây là các biến categorical), đảm bảo rằng các nhóm có ý nghĩa thống kê và mang lại sự phân tách tốt nhất giữa các lớp.\n",
    "- <b>Bước 5 </b>: Encoding các biến categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366dfbb",
   "metadata": {},
   "source": [
    "### APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train= pd.read_csv('../dseb63_final_project_DP_dataset/dseb63_application_train.csv')\n",
    "application_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_test = pd.read_csv('../dseb63_final_project_DP_dataset/dseb63_application_test.csv')\n",
    "application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column 'Unnamed: 0'\n",
    "application_train.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "application_test.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d48147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Anomolies\n",
    "application_train['DAYS_EMPLOYED'][application_train['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "application_test['DAYS_EMPLOYED'][application_test['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "\n",
    "application_train['OBS_30_CNT_SOCIAL_CIRCLE'][application_train['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "application_train['OBS_60_CNT_SOCIAL_CIRCLE'][application_train['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "application_test['OBS_30_CNT_SOCIAL_CIRCLE'][application_test['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "application_test['OBS_60_CNT_SOCIAL_CIRCLE'][application_test['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = remove_missing_col(application_train, 66)\n",
    "application_test = remove_missing_col(application_test, 66.3)\n",
    "\n",
    "application_train = fill_nan(application_train)\n",
    "application_test = fill_nan(application_test)\n",
    "\n",
    "day_cols = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH']\n",
    "years_cols = ['AGE', 'YEARS_EMPLOYED', 'YEARS_REGISTRATION', 'YEARS_ID_PUBLISH' ]\n",
    "\n",
    "application_train = create_day_to_year(application_train,day_cols, years_cols)\n",
    "application_test = create_day_to_year(application_test,day_cols, years_cols)\n",
    "\n",
    "application_train = drop_column_unique_value(application_train)\n",
    "application_test = drop_column_unique_value(application_test)\n",
    "\n",
    "application_train = replace_outlier(application_train)\n",
    "application_test = replace_outlier(application_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb44678",
   "metadata": {},
   "source": [
    "## Create Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numeric feature Engineering\n",
    "def numeric_feature_engineer(data):\n",
    "    \n",
    "    #income and credit features\n",
    "    data['CREDIT_INCOME_RATIO'] = data['AMT_CREDIT'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    data['CREDIT_ANNUITY_RATIO'] = data['AMT_CREDIT'] / (data['AMT_ANNUITY'] + 0.00001)\n",
    "    data['ANNUITY_INCOME_RATIO'] = data['AMT_ANNUITY'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    data['INCOME_ANNUITY_DIFF'] = data['AMT_INCOME_TOTAL'] - data['AMT_ANNUITY']\n",
    "    data['CREDIT_GOODS_RATIO'] = data['AMT_CREDIT'] / (data['AMT_GOODS_PRICE'] + 0.00001)\n",
    "    data['CREDIT_GOODS_DIFF'] = data['AMT_CREDIT'] - data['AMT_GOODS_PRICE'] + 0.00001\n",
    "    data['GOODS_INCOME_RATIO'] = data['AMT_GOODS_PRICE'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    data['PAYMENT_RATE'] = data['AMT_ANNUITY'] / (data['AMT_CREDIT'] + 0.00001)\n",
    "    data['INCOME_CREDIT_PERC'] =  data['AMT_INCOME_TOTAL'] / (data['AMT_CREDIT'] + 0.00001)\n",
    "    data['INCOME_TO_EMPLOYED_RATIO'] = data['AMT_INCOME_TOTAL'] / (data['YEARS_EMPLOYED'] + 0.00001)\n",
    "\n",
    "\n",
    "    #age ratios and diffs\n",
    "    data['AGE_EMPLOYED_DIFF'] = data['AGE'] - data['YEARS_EMPLOYED']\n",
    "    data['EMPLOYED_TO_AGE_RATIO'] = data['YEARS_EMPLOYED'] / (data['AGE'] + 0.00001)\n",
    "    data['INCOME_TO_BIRTH_RATIO'] = data['AMT_INCOME_TOTAL'] / (data['AGE'] + 0.00001)      \n",
    "    data['ID_TO_BIRTH_RATIO'] = data['YEARS_ID_PUBLISH'] / (data['AGE'] + 0.00001)\n",
    "  \n",
    "    #car ratios\n",
    "    data['CAR_EMPLOYED_DIFF'] = data['OWN_CAR_AGE'] - data['YEARS_EMPLOYED']\n",
    "    data['CAR_EMPLOYED_RATIO'] = data['OWN_CAR_AGE'] / (data['YEARS_EMPLOYED']+0.00001)\n",
    "    data['CAR_AGE_DIFF'] = data['AGE'] - data['OWN_CAR_AGE']\n",
    "    data['CAR_AGE_RATIO'] = data['OWN_CAR_AGE'] / (data['AGE'] + 0.00001)\n",
    "    \n",
    "    #flag contacts sum\n",
    "    data['FLAG_CONTACTS_SUM'] =  data['FLAG_EMP_PHONE'] + data['FLAG_WORK_PHONE'] +  data['FLAG_PHONE'] + data['FLAG_EMAIL']\n",
    "\n",
    "    #family members\n",
    "    data['CNT_NON_CHILDREN'] = data['CNT_FAM_MEMBERS'] - data['CNT_CHILDREN']\n",
    "    data['PER_CAPITA_INCOME'] = data['AMT_INCOME_TOTAL'] / (data['CNT_FAM_MEMBERS'] + 1)\n",
    "    data['CHILDREN_RATIO'] =  data['CNT_CHILDREN'] / (data['CNT_FAM_MEMBERS'] + 1)\n",
    "\n",
    "\n",
    "    #region ratings\n",
    "    data['REGIONS_RATING_INCOME_MUL'] = (data['REGION_RATING_CLIENT'] + data['REGION_RATING_CLIENT_W_CITY']) * data['AMT_INCOME_TOTAL'] / 2\n",
    "    data['REGION_RATING_MAX'] = [max(ele1, ele2) for ele1, ele2 in zip(data['REGION_RATING_CLIENT'], data['REGION_RATING_CLIENT_W_CITY'])]\n",
    "    data['REGION_RATING_MAX'] = [min(ele1, ele2) for ele1, ele2 in zip(data['REGION_RATING_CLIENT'], data['REGION_RATING_CLIENT_W_CITY'])]\n",
    "    data['REGION_RATING_MEAN'] = (data['REGION_RATING_CLIENT'] + data['REGION_RATING_CLIENT_W_CITY']) / 2\n",
    "    data['REGION_RATING_MUL'] = data['REGION_RATING_CLIENT'] * data['REGION_RATING_CLIENT_W_CITY']\n",
    "    \n",
    "    #flag regions\n",
    "    data['FLAG_REGIONS'] =  data['REG_REGION_NOT_WORK_REGION'] + data['LIVE_REGION_NOT_WORK_REGION'] + data['REG_CITY_NOT_LIVE_CITY'] + \\\n",
    "                            data['REG_CITY_NOT_WORK_CITY'] + data['LIVE_CITY_NOT_WORK_CITY']\n",
    "    \n",
    "    #ext_sources\n",
    "    data['EXT_SOURCE_MEAN'] = (data['EXT_SOURCE_1'] + data['EXT_SOURCE_2'] + data['EXT_SOURCE_3'] ) / 3\n",
    "    data['EXT_SOURCE_MUL'] = data['EXT_SOURCE_1'] * data['EXT_SOURCE_2'] * data['EXT_SOURCE_3'] \n",
    "    data['EXT_SOURCE_MAX'] = [max(ele1,ele2,ele3) for ele1, ele2, ele3 in zip(data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "    data['EXT_SOURCE_MIN'] = [min(ele1,ele2,ele3) for ele1, ele2, ele3 in zip(data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "    data['EXT_SOURCE_VAR'] = [np.var([ele1,ele2,ele3]) for ele1, ele2, ele3 in zip(data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "    data['WEIGHTED_EXT_SOURCE'] =  data['EXT_SOURCE_1'] * 2 + data['EXT_SOURCE_2'] * 3 + data['EXT_SOURCE_3'] * 4\n",
    "    \n",
    "    #OBS And DEF\n",
    "    data['OBS_30_60_SUM'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] + data['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "    data['DEF_30_60_SUM'] = data['DEF_30_CNT_SOCIAL_CIRCLE'] + data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    data['OBS_DEF_30_MUL'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] *  data['DEF_30_CNT_SOCIAL_CIRCLE']\n",
    "    data['OBS_DEF_60_MUL'] = data['OBS_60_CNT_SOCIAL_CIRCLE'] *  data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    data['SUM_OBS_DEF_ALL'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] + data['DEF_30_CNT_SOCIAL_CIRCLE'] + data[\n",
    "        'OBS_60_CNT_SOCIAL_CIRCLE'] + data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    data['OBS_30_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['OBS_30_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    data['OBS_60_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['OBS_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    data['DEF_30_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['DEF_30_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    data['DEF_60_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['DEF_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    \n",
    "    #Flag Documents combined\n",
    "    data['SUM_FLAGS_DOCUMENTS'] = data['FLAG_DOCUMENT_3'] + data['FLAG_DOCUMENT_6'] + data['FLAG_DOCUMENT_8']         \n",
    "\n",
    "    #enquires\n",
    "    \n",
    "    data['AMT_ENQ_SUM'] = data['AMT_REQ_CREDIT_BUREAU_HOUR'] + data['AMT_REQ_CREDIT_BUREAU_DAY'] + data['AMT_REQ_CREDIT_BUREAU_WEEK'] + data[\n",
    "        'AMT_REQ_CREDIT_BUREAU_MON'] + data['AMT_REQ_CREDIT_BUREAU_QRT'] + data['AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "    data['ENQ_CREDIT_RATIO'] = data['AMT_ENQ_SUM'] / (data['AMT_CREDIT'] + 0.00001)\n",
    "    \n",
    " \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = numeric_feature_engineer(application_train)\n",
    "application_test = numeric_feature_engineer(application_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features shape: ', application_train.shape)\n",
    "print('Testing Features shape: ', application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e313c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning process\n",
    "target_var = application_train['TARGET']\n",
    "application_train.drop('TARGET', axis = 1, inplace = True)\n",
    "cate_cols = application_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "columns = application_train.columns.tolist()\n",
    "binning_process = BinningProcess(columns, categorical_variables=cate_cols, max_n_prebins=30)\n",
    "\n",
    "binning_process.fit(application_train, target_var)\n",
    "\n",
    "# Transform train and test\n",
    "train_binned = binning_process.transform(application_train, metric_missing=0.05)\n",
    "train_binned.columns = [f'{col}_BIN' for col in train_binned.columns]\n",
    "train_binned['SK_ID_CURR'] = application_train['SK_ID_CURR']\n",
    "\n",
    "test_binned = binning_process.transform(application_test, metric_missing=0.05)\n",
    "test_binned.columns = [f'{col}_BIN' for col in test_binned.columns]\n",
    "test_binned['SK_ID_CURR'] = application_test['SK_ID_CURR']\n",
    "\n",
    "# Merge bin column to original dataframe\n",
    "application_train = application_train.merge(train_binned, on = 'SK_ID_CURR', how = 'left')\n",
    "application_test = application_test.merge(test_binned, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "application_train['TARGET'] = target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features shape: ', application_train.shape)\n",
    "print('Testing Features shape: ', application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = encode(application_train)\n",
    "application_test = encode(application_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11667ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features shape: ', application_train.shape)\n",
    "print('Testing Features shape: ', application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17824098",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aligning train and test data\n",
    "train_labels = application_train['TARGET']\n",
    "\n",
    "# Align the training and testing data, keep only columns present in both dataframes\n",
    "application_train, application_test = application_train.align(application_test, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target back in\n",
    "application_train['TARGET'] = train_labels\n",
    "\n",
    "print('Training Features shape: ', application_train.shape)\n",
    "print('Testing Features shape: ', application_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train.to_csv('application_train_final.csv', index = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ef0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_test.to_csv('application_test_final.csv', index = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe8e11",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the SK_ID_CURR from training and test data\n",
    "application_train = application_train.drop(['SK_ID_CURR'], axis = 1)\n",
    "skid_test = application_test.pop('SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(C = 0.1)\n",
    "train, test = train_test_split(application_train,test_size=.25,random_state = 123)\n",
    "\n",
    "#separating dependent and independent variables\n",
    "train_x1 = train[[i for i in train.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "train_y1 = train[[\"TARGET\"]]\n",
    "\n",
    "test_x1 = test[[i for i in test.columns if i not in ['SK_ID_CURR'] + [ 'TARGET']]]\n",
    "test_y1 = test[[\"TARGET\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x1 = scaler.fit_transform(train_x1)\n",
    "test_x1 = scaler.fit_transform(test_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b39a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(train_x1, train_y1)\n",
    "log_reg_pred = log_reg.predict_proba(test_x1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_roc = roc_auc_score(test_y1,log_reg_pred)\n",
    "gini = 2 * auc_roc - 1\n",
    "print(f\"AUC-ROC Score: {auc_roc}\")\n",
    "print(f\"Gini Score: {gini}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
